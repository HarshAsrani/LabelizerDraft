{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwrV512Wds1g"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmVpIfmvds1j",
    "outputId": "b250c696-2bfc-4390-c6ef-329f4d091d4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranabislam/miniconda3/envs/markupmna/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import os \n",
    "import glob\n",
    "from transformers import MarkupLMProcessor\n",
    "from transformers import MarkupLMForTokenClassification\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import utils\n",
    "# import input_pipeline as ip\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "1. create_raw_dataset: takes the tagged csvs and creates a dict of \n",
    "    xpaths, nodes, node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_dataset(tagged_csv_path, id2label, label2id, is_train=True):\n",
    "    \"\"\"Preprocesses the tagged csvs in the format required by MarkupLM\"\"\"\n",
    "\n",
    "    tagged_df = pd.read_csv(tagged_csv_path)\n",
    "\n",
    "    # in train mode we expect text and xpaths that are highlighted \n",
    "    # by an annotator\n",
    "    if is_train:\n",
    "        col_list = [\"nodes\", \"xpaths\", \"node_labels\"]\n",
    "        \n",
    "        tagged_df[\"highlighted_xpaths\"] = tagged_df[\"highlighted_xpaths\"].fillna(\n",
    "            tagged_df[\"xpaths\"]\n",
    "        )\n",
    "        tagged_df[\"highlighted_segmented_text\"] = tagged_df[\n",
    "            \"highlighted_segmented_text\"\n",
    "        ].fillna(tagged_df[\"text\"])\n",
    "\n",
    "        # drop non-ASCII chars\n",
    "        tagged_df[\"highlighted_segmented_text\"] = (\n",
    "            tagged_df[\"highlighted_segmented_text\"]\n",
    "            .str.encode(\"ascii\", errors=\"ignore\")\n",
    "            .str.decode(\"ascii\")\n",
    "        )\n",
    "\n",
    "        # rename columns to match MarkupLM convention\n",
    "        tagged_df = tagged_df.rename(\n",
    "            columns={\n",
    "                \"highlighted_xpaths\": \"xpaths\",\n",
    "                \"highlighted_segmented_text\": \"nodes\",\n",
    "                \"tagged_sequence\": \"node_labels\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # convert node labels to integer values\n",
    "        tagged_df[\"node_labels\"] = tagged_df[\"node_labels\"].apply(\n",
    "            lambda label: label2id[label]\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        col_list = [\"nodes\", \"xpaths\"]\n",
    "        \n",
    "        # rename columns to match MarkupLM convention\n",
    "        tagged_df = tagged_df.rename(\n",
    "            columns={\n",
    "                \"xpaths\": \"xpaths\",\n",
    "                \"text\": \"nodes\",\n",
    "            },\n",
    "        )     \n",
    "    \n",
    "    tagged_output = tagged_df.loc[:, col_list].to_dict(orient=\"list\")\n",
    "\n",
    "    # convert each key to a list of lists just like the MarkupLM\n",
    "    # pipeline requires\n",
    "    for k, v in tagged_output.items():\n",
    "        tagged_output[k] = [v]\n",
    "\n",
    "    return tagged_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkupLMDataset(Dataset):\n",
    "    \"\"\"Dataset for token classification with MarkupLM.\"\"\"\n",
    "\n",
    "    def __init__(self, data, processor=None, max_length=512, is_train=True):\n",
    "        self.data = data\n",
    "        self.is_train = is_train\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        self.get_encoding_windows()\n",
    "        \n",
    "\n",
    "    def get_encoding_windows(self):\n",
    "        \"\"\"Splits the tokenized input into windows of 512 tokens\"\"\"\n",
    "                \n",
    "        for item in self.data:            \n",
    "            if self.is_train:\n",
    "                nodes, xpaths, node_labels = (\n",
    "                    item[\"nodes\"],\n",
    "                    item[\"xpaths\"],\n",
    "                    item['node_labels']\n",
    "                )\n",
    "            else:\n",
    "                nodes, xpaths, node_labels = (\n",
    "                    item[\"nodes\"],\n",
    "                    item[\"xpaths\"],\n",
    "                    None\n",
    "                )                \n",
    "            \n",
    "            # provide encoding to processor\n",
    "            encoding = self.processor(\n",
    "                nodes=nodes,\n",
    "                xpaths=xpaths,\n",
    "                node_labels=node_labels,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=False,\n",
    "                return_offsets_mapping=True\n",
    "            )\n",
    "\n",
    "            # remove batch dimension\n",
    "            encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "\n",
    "            # chunk up the encoding sequences to that it is less than the \n",
    "            # max input length of 512 tokens\n",
    "            if not self.is_train:\n",
    "                #num_tokens = len(item['nodes'][0])\n",
    "                num_tokens = len(encoding['input_ids'])\n",
    "                \n",
    "                for idx in range(0, num_tokens, self.max_length):\n",
    "                    batch_encoding = {}\n",
    "                    for k, v in encoding.items():\n",
    "                        batch_encoding[k] = v[idx: idx + self.max_length]\n",
    "\n",
    "                    self.encodings.append(batch_encoding)                    \n",
    "                    continue\n",
    "            \n",
    "            else:\n",
    "                if len(encoding[\"input_ids\"]) <= self.max_length:                    \n",
    "                    self.encodings.append(encoding)\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    batch_encoding = {}\n",
    "\n",
    "                    start_idx, end_idx = 0, self.max_length\n",
    "\n",
    "                    while end_idx < len(encoding[\"input_ids\"]):\n",
    "                        # decrement the end_idx by 1 until the label is not -100\n",
    "                        while encoding[\"labels\"][end_idx] == -100:\n",
    "                            end_idx = end_idx - 1\n",
    "\n",
    "                            # if the end idx is equal to the start idx meaning\n",
    "                            # we don't encounter a non -100 token,\n",
    "                            # we set window size as the max_length\n",
    "                            if end_idx == start_idx:\n",
    "                                end_idx = start_idx + self.max_length\n",
    "                                break\n",
    "\n",
    "                        for k, v in encoding.items():\n",
    "                            batch_encoding[k] = v[start_idx:end_idx]\n",
    "\n",
    "                        self.encodings.append(batch_encoding)\n",
    "                        batch_encoding = {}\n",
    "\n",
    "                        # update the pointers\n",
    "                        start_idx = end_idx\n",
    "                        end_idx = end_idx + self.max_length\n",
    "\n",
    "                    # collect the remaining tokens\n",
    "                    for k, v in encoding.items():\n",
    "                        batch_encoding[k] = v[start_idx:]\n",
    "\n",
    "                    if batch_encoding:\n",
    "                        self.encodings.append(batch_encoding)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # first, get nodes, xpaths and node labels\n",
    "        item = self.encodings[idx]\n",
    "\n",
    "        # pad the encodings to max_length of 512 tokens\n",
    "        padded_item = self.processor.tokenizer.pad(\n",
    "            item, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return padded_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference Loop and Main Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_loop(dataloader, model, device, config, processor):\n",
    "    '''Runs eval loop for entire dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader: torch.utils.data.DataLoader: iterator over Dataset object\n",
    "        model: transformers.PreTrainedModel. fine-tuned MarkupLM model\n",
    "        device: torch.device. Specifies whether GPU is available for computation\n",
    "        label_list: list. List of labels used to train the MarkupLM model\n",
    "        config: dict. Contains user-provided params and args\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    results = {\"nodes\": [], \"preds\": []}\n",
    "    for batch in tqdm(dataloader, desc='inference_loop'):\n",
    "        # get the inputs;\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # if ablation mode is set to true then\n",
    "        # either mask the xpaths or shuffle them\n",
    "        if config[\"ablation\"][\"run_ablation\"]:\n",
    "            inputs = utils.ablation(config, inputs)\n",
    "\n",
    "        # get the offset mapping. It contains the spans of the \n",
    "        # words that were split during tokenization. \n",
    "        # Info present at a token level\n",
    "        offset_mapping = inputs.pop('offset_mapping').squeeze().tolist()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        #logits[1] = outputs.logits\n",
    "        #print(predictions.squeeze().tolist())\n",
    "        pred_labels = [model.config.id2label[idx] for idx in predictions.squeeze().tolist()]\n",
    "        \n",
    "        input_ids = inputs['input_ids'].detach().numpy().flatten().tolist()\n",
    "        input_word_pieces = [processor.decode([id]) for id in input_ids]\n",
    "        \n",
    "        \n",
    "        # input_ids = [x for x in input_ids if x not in special_tokens]\n",
    "        # print(input_ids)\n",
    "        results['nodes'].append(input_word_pieces)\n",
    "        results['preds'].append(pred_labels)\n",
    "                                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, test_data, model_ckpt_path=None, is_train=False):\n",
    "    '''Main execution of script'''\n",
    "    # get the  list of labels along with the label to id mapping and\n",
    "    # reverse mapping\n",
    "    label_list, id2label, label2id = utils.get_label_list(config)\n",
    "    \n",
    "    # define the processor and model\n",
    "    if config[\"model\"][\"use_large_model\"]:\n",
    "        processor = MarkupLMProcessor.from_pretrained(\n",
    "            \"microsoft/markuplm-large\",\n",
    "            only_label_first_subword=config['model']['label_only_first_subword']\n",
    "        )\n",
    "        model = MarkupLMForTokenClassification.from_pretrained(\n",
    "            \"microsoft/markuplm-large\", id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        processor = MarkupLMProcessor.from_pretrained(\n",
    "            \"microsoft/markuplm-base\",\n",
    "            only_label_first_subword=config['model']['label_only_first_subword'],\n",
    "        )\n",
    "        model = MarkupLMForTokenClassification.from_pretrained(\n",
    "            \"microsoft/markuplm-base\", id2label=id2label, label2id=label2id\n",
    "        )\n",
    "        \n",
    "    if model_ckpt_path is not None:\n",
    "        model_ckpt = torch.load(model_ckpt_path, \n",
    "                                map_location='cpu')\n",
    "        \n",
    "        model.load_state_dict(model_ckpt)\n",
    "        \n",
    "\n",
    "    processor.parse_html = False\n",
    "    \n",
    "    # convert the input dataset\n",
    "    # to torch datasets. Create the dataloaders as well\n",
    "    test_dataset = MarkupLMDataset(\n",
    "        data=test_data,\n",
    "        processor=processor,\n",
    "        max_length=config[\"model\"][\"max_length\"],\n",
    "        is_train=is_train\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model.to(device)  # move to GPU if available\n",
    "\n",
    "    print(\"*\" * 50)\n",
    "    print(f'Running Inference Loop!')\n",
    "    print(\"*\" * 50)\n",
    "\n",
    "    # run inference loop\n",
    "    results = run_inference_loop(test_dataloader, model, device, \n",
    "                                     config, processor)\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the input params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './configs/config.yaml'\n",
    "\n",
    "test_contract_dir = \"../contracts/test\"\n",
    "val_contract_dir = \"../contracts/val\"\n",
    "train_contract_dir = \"../contracts/train\"\n",
    "\n",
    "# if loading from ckpt then change the line below\n",
    "#model_ckpt_path = \"/Users/pranabislam/Desktop/research/MarkupMnA-Markup-Based-Segmentation-of-MnA-Agreements/markup-mna/models/pretrained_models/markuplm_base_model_ablation_shuffle_num_contract_100pct_f1-0.871.pt\"\n",
    "model_ckpt_path = \"/Users/pranabislam/Desktop/research/MarkupMnA-Markup-Based-Segmentation-of-MnA-Agreements/markup-mna/models/pretrained_models/markuplm_base_model_num_contracts-121_epoch-10_f1-0.903.pt\"\n",
    "max_length = 512\n",
    "test_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the config file \n",
    "with open(config_path, 'r') as yml:\n",
    "    config = yaml.safe_load(yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list, id2label, label2id = utils.get_label_list(config)\n",
    "num_labels = len(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 in test dir\n"
     ]
    }
   ],
   "source": [
    "test_contracts = glob.glob(os.path.join(test_contract_dir, \"*.csv\"))\n",
    "\n",
    "#test_contracts = [test_contracts[2]]\n",
    "\n",
    "print(f\"Found {len(test_contracts)} in test dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../contracts/test/contract_69.csv',\n",
       " '../contracts/test/contract_81.csv',\n",
       " '../contracts/test/contract_85.csv',\n",
       " '../contracts/test/contract_46.csv',\n",
       " '../contracts/test/contract_84.csv',\n",
       " '../contracts/test/contract_92.csv',\n",
       " '../contracts/test/contract_37.csv',\n",
       " '../contracts/test/contract_104.csv',\n",
       " '../contracts/test/contract_35.csv',\n",
       " '../contracts/test/contract_31.csv',\n",
       " '../contracts/test/contract_128.csv',\n",
       " '../contracts/test/contract_129.csv',\n",
       " '../contracts/test/contract_30.csv',\n",
       " '../contracts/test/contract_116.csv',\n",
       " '../contracts/test/contract_137.csv',\n",
       " '../contracts/test/contract_61.csv',\n",
       " '../contracts/test/contract_67.csv',\n",
       " '../contracts/test/contract_72.csv',\n",
       " '../contracts/test/contract_58.csv',\n",
       " '../contracts/test/contract_59.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['69',\n",
       " '81',\n",
       " '85',\n",
       " '46',\n",
       " '84',\n",
       " '92',\n",
       " '37',\n",
       " '104',\n",
       " '35',\n",
       " '31',\n",
       " '128',\n",
       " '129',\n",
       " '30',\n",
       " '116',\n",
       " '137',\n",
       " '61',\n",
       " '67',\n",
       " '72',\n",
       " '58',\n",
       " '59']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.split('_')[-1].split('.csv')[0] for x in test_contracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = [] \n",
    "for tagged_path in test_contracts:\n",
    "    tagged_output = create_raw_dataset(tagged_path, \n",
    "                                       id2label=id2label, \n",
    "                                       label2id=label2id,\n",
    "                                       is_train=False)\n",
    "\n",
    "    all_test_data.append(tagged_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repeat_xpaths(processor, row):\n",
    "    '''\n",
    "    Given a row in a dataset, create a list of repeating xpaths with length of the number of tokens\n",
    "    in the text row\n",
    "    '''\n",
    "    enc = processor(\n",
    "        nodes=[row.text],\n",
    "        xpaths=[row.xpaths],\n",
    "        #node_labels=node_labels,\n",
    "        #padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=False,\n",
    "    )\n",
    "    \n",
    "    num_tokens = enc.input_ids.shape[1] - 2\n",
    "    tokens = [processor.decode(x, skip_special_tokens=False) for x in enc.input_ids[0][1:-1]]\n",
    "    \n",
    "    return [[row.xpaths] * num_tokens, tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/133 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 133/133 [01:17<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/208 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 208/208 [02:11<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/118 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 118/118 [01:13<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/180 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 180/180 [01:42<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/432 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 432/432 [04:19<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/159 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 159/159 [01:32<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/142 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 142/142 [01:30<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/126 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 126/126 [01:17<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/144 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 144/144 [01:33<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/144 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 144/144 [01:33<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/181 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 181/181 [01:57<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/125 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 125/125 [01:21<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/107 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 107/107 [01:08<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/123 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 123/123 [01:21<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/131 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 131/131 [01:29<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/133 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 133/133 [01:30<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/120 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 120/120 [01:16<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/127 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 127/127 [01:24<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/112 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 112/112 [01:13<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'nrp_cls.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'nrp_cls.dense.bias', 'cls.predictions.transform.dense.bias', 'ptc_cls.bias', 'cls.predictions.decoder.weight', 'markuplm.pooler.dense.bias', 'nrp_cls.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight', 'cls.predictions.transform.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'markuplm.pooler.dense.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                    | 0/97 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|███████████████████████████| 97/97 [01:02<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 59\n"
     ]
    }
   ],
   "source": [
    "processor = MarkupLMProcessor.from_pretrained(\n",
    "    \"microsoft/markuplm-base\",\n",
    "    only_label_first_subword=True\n",
    ")\n",
    "processor.parse_html = False\n",
    "\n",
    "for i, test_data in enumerate(all_test_data):\n",
    "    \n",
    "    test_data = [test_data]\n",
    "    contract_num = test_contracts[i].split('_')[-1].split('.csv')[0]\n",
    "    inference_csv_path = test_contracts[i]\n",
    "    \n",
    "    results = main(config, test_data, model_ckpt_path=model_ckpt_path,is_train=False)\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df = df.explode(['nodes', 'preds']).reset_index(drop=True)\n",
    "    \n",
    "    inference_csv = pd.read_csv(inference_csv_path)[['xpaths','text']].copy()\n",
    "    \n",
    "    g = inference_csv.apply(lambda row: create_repeat_xpaths(processor, row), axis=1)\n",
    "    \n",
    "    inference_csv['xpaths_list'] = g.apply(lambda x: x[0])\n",
    "    inference_csv['text_list'] = g.apply(lambda x: x[1])\n",
    "    \n",
    "    exploded = inference_csv.explode(['xpaths_list', 'text_list']).reset_index(drop=True)\n",
    "    \n",
    "    filter_df_based_on_start_and_end_token = df.query(\"nodes == '<s>' | nodes == '</s>'\")\n",
    "    start_idx = filter_df_based_on_start_and_end_token.index.min() + 1\n",
    "    end_idx = filter_df_based_on_start_and_end_token.index.max()\n",
    "    \n",
    "    preds = df.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    exploded_partial = exploded.copy()\n",
    "    \n",
    "    exploded_partial['nodes'] = preds['nodes'].to_list()\n",
    "    exploded_partial['preds'] = preds['preds'].to_list()\n",
    "    \n",
    "    print(sum(exploded_partial.text_list == exploded_partial.nodes) == len(exploded_partial))\n",
    "    \n",
    "    print((exploded_partial['xpaths'] == exploded_partial['xpaths_list']).sum() == len(exploded_partial))\n",
    "    \n",
    "    exploded_partial[['xpaths','text','nodes','preds']].to_json(f'predictions_contract_{contract_num}.json',orient='records')\n",
    "    print('contract done', contract_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 in val dir\n"
     ]
    }
   ],
   "source": [
    "val_contracts = glob.glob(os.path.join(val_contract_dir, \"*.csv\"))\n",
    "\n",
    "print(f\"Found {len(val_contracts)} in val dir\")\n",
    "\n",
    "all_data = [] \n",
    "for tagged_path in val_contracts:\n",
    "    tagged_output = create_raw_dataset(tagged_path, \n",
    "                                       id2label=id2label, \n",
    "                                       label2id=label2id,\n",
    "                                       is_train=False)\n",
    "\n",
    "    all_data.append(tagged_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MarkupLMProcessor.from_pretrained(\n",
    "    \"microsoft/markuplm-base\",\n",
    "    only_label_first_subword=True\n",
    ")\n",
    "processor.parse_html = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/133 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop: 100%|█████████████████████████| 133/133 [01:06<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/105 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 105/105 [00:56<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/156 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 156/156 [01:20<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/141 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 141/141 [01:16<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/123 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 123/123 [01:05<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/160 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 160/160 [01:21<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/118 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 118/118 [01:00<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/186 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 186/186 [01:32<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/107 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 107/107 [00:53<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/168 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 168/168 [01:23<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 140\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(all_data):\n",
    "    \n",
    "    data = [data]\n",
    "    contract_num = val_contracts[i].split('_')[-1].split('.csv')[0]\n",
    "    inference_csv_path = val_contracts[i]\n",
    "    \n",
    "    results = main(config, data, model_ckpt_path=model_ckpt_path,is_train=False)\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df = df.explode(['nodes', 'preds']).reset_index(drop=True)\n",
    "    \n",
    "    inference_csv = pd.read_csv(inference_csv_path)[['xpaths','text']].copy()\n",
    "    \n",
    "    g = inference_csv.apply(lambda row: create_repeat_xpaths(processor, row), axis=1)\n",
    "    \n",
    "    inference_csv['xpaths_list'] = g.apply(lambda x: x[0])\n",
    "    inference_csv['text_list'] = g.apply(lambda x: x[1])\n",
    "    \n",
    "    exploded = inference_csv.explode(['xpaths_list', 'text_list']).reset_index(drop=True)\n",
    "    \n",
    "    filter_df_based_on_start_and_end_token = df.query(\"nodes == '<s>' | nodes == '</s>'\")\n",
    "    start_idx = filter_df_based_on_start_and_end_token.index.min() + 1\n",
    "    end_idx = filter_df_based_on_start_and_end_token.index.max()\n",
    "    \n",
    "    preds = df.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    exploded_partial = exploded.copy()\n",
    "    \n",
    "    exploded_partial['nodes'] = preds['nodes'].to_list()\n",
    "    exploded_partial['preds'] = preds['preds'].to_list()\n",
    "    \n",
    "    print(sum(exploded_partial.text_list == exploded_partial.nodes) == len(exploded_partial))\n",
    "    \n",
    "    print((exploded_partial['xpaths'] == exploded_partial['xpaths_list']).sum() == len(exploded_partial))\n",
    "    \n",
    "    exploded_partial[['xpaths','text','nodes','preds']].to_json(f'val_inference/predictions_contract_{contract_num}.json',orient='records')\n",
    "    print('contract done', contract_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121 in train dir\n"
     ]
    }
   ],
   "source": [
    "train_contracts = glob.glob(os.path.join(train_contract_dir, \"*.csv\"))\n",
    "\n",
    "print(f\"Found {len(train_contracts)} in train dir\")\n",
    "\n",
    "all_data = [] \n",
    "for tagged_path in train_contracts:\n",
    "    tagged_output = create_raw_dataset(tagged_path, \n",
    "                                       id2label=id2label, \n",
    "                                       label2id=label2id,\n",
    "                                       is_train=False)\n",
    "\n",
    "    all_data.append(tagged_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/114 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 114/114 [00:55<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/106 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 106/106 [00:51<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/163 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 163/163 [01:22<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/310 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 310/310 [02:42<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/121 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 121/121 [01:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/119 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 119/119 [00:59<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/183 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 183/183 [01:31<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/135 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 135/135 [01:07<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/174 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 174/174 [01:26<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/101 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 101/101 [00:50<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/155 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 155/155 [01:15<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/139 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 139/139 [01:08<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/142 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 142/142 [01:11<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/156 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 156/156 [01:26<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/127 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 127/127 [01:10<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/133 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 133/133 [01:16<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/145 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 145/145 [01:25<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/141 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 141/141 [01:30<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/169 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 169/169 [01:51<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/144 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 144/144 [01:43<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/184 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 184/184 [02:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/194 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 194/194 [02:36<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/112 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 112/112 [01:30<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/138 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 138/138 [01:21<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/107 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 107/107 [00:53<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/184 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 184/184 [01:28<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                    | 0/99 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|███████████████████████████| 99/99 [00:49<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/101 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 101/101 [00:49<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/107 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 107/107 [00:52<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/133 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 133/133 [01:04<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/132 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 132/132 [01:04<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/139 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 139/139 [01:07<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/157 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 157/157 [01:17<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/160 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 160/160 [01:24<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/122 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 122/122 [01:07<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/173 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 173/173 [01:36<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/207 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 207/207 [01:52<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/157 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 157/157 [01:26<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/100 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 100/100 [00:54<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                    | 0/97 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|███████████████████████████| 97/97 [00:56<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/141 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 141/141 [01:38<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/153 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 153/153 [01:27<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/146 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 146/146 [01:24<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/142 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 142/142 [01:20<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/175 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 175/175 [01:34<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/124 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 124/124 [01:08<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/140 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 140/140 [01:13<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/125 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 125/125 [01:07<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/132 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 132/132 [01:09<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/186 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 186/186 [01:37<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/148 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 148/148 [01:17<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/150 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 150/150 [01:18<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/106 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 106/106 [00:56<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/405 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 405/405 [03:48<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/199 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 199/199 [01:53<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/129 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 129/129 [01:16<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/111 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 111/111 [01:05<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/135 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 135/135 [01:14<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/136 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 136/136 [01:20<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/117 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 117/117 [01:10<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/112 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 112/112 [01:02<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/133 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 133/133 [01:13<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/157 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 157/157 [01:26<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/151 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 151/151 [01:28<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/197 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 197/197 [01:52<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/132 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 132/132 [01:16<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/182 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 182/182 [01:44<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/145 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 145/145 [01:22<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/129 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 129/129 [01:13<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/122 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 122/122 [01:09<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/111 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 111/111 [01:04<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/104 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 104/104 [00:59<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/142 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 142/142 [01:20<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/145 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 145/145 [01:23<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                    | 0/85 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|███████████████████████████| 85/85 [00:49<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                    | 0/93 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|███████████████████████████| 93/93 [00:54<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/139 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 139/139 [01:19<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/140 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 140/140 [01:18<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/169 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 169/169 [01:41<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/200 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 200/200 [01:49<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/151 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 151/151 [01:27<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/151 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 151/151 [01:26<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/165 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 165/165 [01:33<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/119 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 119/119 [01:08<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/171 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 171/171 [01:39<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/104 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 104/104 [01:00<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/153 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 153/153 [01:28<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/144 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 144/144 [01:22<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/139 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 139/139 [01:19<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/109 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 109/109 [01:02<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/141 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 141/141 [01:21<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/144 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 144/144 [01:21<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/127 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 127/127 [01:13<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/133 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 133/133 [01:16<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/149 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 149/149 [01:24<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/131 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 131/131 [01:16<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/136 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 136/136 [01:17<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/136 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 136/136 [01:18<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/164 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 164/164 [01:33<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/155 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 155/155 [01:27<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/136 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 136/136 [01:17<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/328 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 328/328 [03:01<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/126 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 126/126 [01:09<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/132 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 132/132 [01:12<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/130 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 130/130 [01:13<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/130 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 130/130 [01:11<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/171 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 171/171 [01:44<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/142 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 142/142 [01:23<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/121 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 121/121 [01:07<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/131 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 131/131 [01:13<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/152 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 152/152 [01:26<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/126 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 126/126 [01:19<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/143 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 143/143 [01:22<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/205 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 205/205 [01:58<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/164 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 164/164 [01:36<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/145 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 145/145 [01:25<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/158 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 158/158 [01:31<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/120 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 120/120 [01:08<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/117 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 117/117 [01:06<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/161 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 161/161 [01:40<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['cls.predictions.bias', 'ptc_cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'nrp_cls.dense.weight', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'nrp_cls.dense.bias', 'nrp_cls.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'nrp_cls.LayerNorm.weight', 'cls.predictions.decoder.weight', 'nrp_cls.decoder.bias', 'ptc_cls.bias', 'nrp_cls.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/138 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 138/138 [01:21<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "contract done 71\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(all_data):\n",
    "    \n",
    "    data = [data]\n",
    "    contract_num = train_contracts[i].split('_')[-1].split('.csv')[0]\n",
    "    inference_csv_path = train_contracts[i]\n",
    "    \n",
    "    results = main(config, data, model_ckpt_path=model_ckpt_path,is_train=False)\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df = df.explode(['nodes', 'preds']).reset_index(drop=True)\n",
    "    \n",
    "    inference_csv = pd.read_csv(inference_csv_path)[['xpaths','text']].copy()\n",
    "    \n",
    "    g = inference_csv.apply(lambda row: create_repeat_xpaths(processor, row), axis=1)\n",
    "    \n",
    "    inference_csv['xpaths_list'] = g.apply(lambda x: x[0])\n",
    "    inference_csv['text_list'] = g.apply(lambda x: x[1])\n",
    "    \n",
    "    exploded = inference_csv.explode(['xpaths_list', 'text_list']).reset_index(drop=True)\n",
    "    \n",
    "    filter_df_based_on_start_and_end_token = df.query(\"nodes == '<s>' | nodes == '</s>'\")\n",
    "    start_idx = filter_df_based_on_start_and_end_token.index.min() + 1\n",
    "    end_idx = filter_df_based_on_start_and_end_token.index.max()\n",
    "    \n",
    "    preds = df.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    exploded_partial = exploded.copy()\n",
    "    \n",
    "    exploded_partial['nodes'] = preds['nodes'].to_list()\n",
    "    exploded_partial['preds'] = preds['preds'].to_list()\n",
    "    \n",
    "    print(sum(exploded_partial.text_list == exploded_partial.nodes) == len(exploded_partial))\n",
    "    \n",
    "    print((exploded_partial['xpaths'] == exploded_partial['xpaths_list']).sum() == len(exploded_partial))\n",
    "    \n",
    "    exploded_partial[['xpaths','text','nodes','preds']].to_json(f'train_inference/predictions_contract_{contract_num}.json',orient='records')\n",
    "    print('contract done', contract_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/markuplm-base were not used when initializing MarkupLMForTokenClassification: ['nrp_cls.decoder.bias', 'markuplm.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'nrp_cls.dense.bias', 'nrp_cls.dense.weight', 'nrp_cls.LayerNorm.bias', 'ptc_cls.weight', 'cls.predictions.bias', 'ptc_cls.bias', 'cls.predictions.transform.LayerNorm.bias', 'markuplm.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'nrp_cls.LayerNorm.weight', 'nrp_cls.decoder.weight']\n",
      "- This IS expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarkupLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Running Inference Loop!\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inference_loop:   0%|                                   | 0/118 [00:00<?, ?it/s]You're using a MarkupLMTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "inference_loop: 100%|█████████████████████████| 118/118 [01:04<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "results = main(config, test_data, model_ckpt_path=model_ckpt_path,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode(['nodes', 'preds']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to match the predictions with the visualizer now (need some way to glue back to xpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xpaths</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/html/body/document/type/sequence</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/html/body/document/type/sequence/filename</td>\n",
       "      <td>d101795dex21.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>EX-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>EX-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Transactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Section 3.4(a)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>U.S. Pension Plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Section 9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2926 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 xpaths               text\n",
       "0                              /html/body/document/type             EX-2.1\n",
       "1                     /html/body/document/type/sequence                  2\n",
       "2            /html/body/document/type/sequence/filename   d101795dex21.htm\n",
       "3     /html/body/document/type/sequence/filename/des...             EX-2.1\n",
       "4     /html/body/document/type/sequence/filename/des...             EX-2.1\n",
       "...                                                 ...                ...\n",
       "2921  /html/body/document/type/sequence/filename/des...       Transactions\n",
       "2922  /html/body/document/type/sequence/filename/des...     Section 3.4(a)\n",
       "2923  /html/body/document/type/sequence/filename/des...  U.S. Pension Plan\n",
       "2924  /html/body/document/type/sequence/filename/des...        Section 9.4\n",
       "2925  /html/body/document/type/sequence/filename/des...          Annex I-4\n",
       "\n",
       "[2926 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference_csv_path = '/Users/pranabislam/Desktop/research/MarkupMnA-Markup-Based-Segmentation-of-MnA-Agreements/contracts/test/contract_81.csv'\n",
    "inference_csv_path = test_contracts[0]\n",
    "inference_csv = pd.read_csv(inference_csv_path)[['xpaths','text']].copy()\n",
    "inference_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MarkupLMProcessor.from_pretrained(\n",
    "    \"microsoft/markuplm-base\",\n",
    "    only_label_first_subword=True\n",
    ")\n",
    "processor.parse_html = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = inference_csv.apply(lambda row: create_repeat_xpaths(processor, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[/html/body/document/type, /html/body/documen...\n",
       "1              [[/html/body/document/type/sequence], [2]]\n",
       "2       [[/html/body/document/type/sequence/filename, ...\n",
       "3       [[/html/body/document/type/sequence/filename/d...\n",
       "4       [[/html/body/document/type/sequence/filename/d...\n",
       "                              ...                        \n",
       "2921    [[/html/body/document/type/sequence/filename/d...\n",
       "2922    [[/html/body/document/type/sequence/filename/d...\n",
       "2923    [[/html/body/document/type/sequence/filename/d...\n",
       "2924    [[/html/body/document/type/sequence/filename/d...\n",
       "2925    [[/html/body/document/type/sequence/filename/d...\n",
       "Length: 2926, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_csv['xpaths_list'] = g.apply(lambda x: x[0])\n",
    "inference_csv['text_list'] = g.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xpaths</th>\n",
       "      <th>text</th>\n",
       "      <th>xpaths_list</th>\n",
       "      <th>text_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60072</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60073</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>nex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60074</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60075</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60076</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60077 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  xpaths       text  \\\n",
       "0                               /html/body/document/type     EX-2.1   \n",
       "1                               /html/body/document/type     EX-2.1   \n",
       "2                               /html/body/document/type     EX-2.1   \n",
       "3                               /html/body/document/type     EX-2.1   \n",
       "4                               /html/body/document/type     EX-2.1   \n",
       "...                                                  ...        ...   \n",
       "60072  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60073  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60074  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60075  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60076  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "\n",
       "                                             xpaths_list text_list  \n",
       "0                               /html/body/document/type        EX  \n",
       "1                               /html/body/document/type         -  \n",
       "2                               /html/body/document/type         2  \n",
       "3                               /html/body/document/type         .  \n",
       "4                               /html/body/document/type         1  \n",
       "...                                                  ...       ...  \n",
       "60072  /html/body/document/type/sequence/filename/des...        An  \n",
       "60073  /html/body/document/type/sequence/filename/des...       nex  \n",
       "60074  /html/body/document/type/sequence/filename/des...         I  \n",
       "60075  /html/body/document/type/sequence/filename/des...         -  \n",
       "60076  /html/body/document/type/sequence/filename/des...         4  \n",
       "\n",
       "[60077 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded = inference_csv.explode(['xpaths_list', 'text_list']).reset_index(drop=True)\n",
    "exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EX</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>s_ssn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60411</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60412</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60413</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60414</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60415</th>\n",
       "      <td>&lt;pad&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60416 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       nodes  preds\n",
       "0        <s>      o\n",
       "1         EX      o\n",
       "2          -    s_n\n",
       "3          2  s_ssn\n",
       "4          .      o\n",
       "...      ...    ...\n",
       "60411  <pad>      o\n",
       "60412  <pad>      o\n",
       "60413  <pad>      o\n",
       "60414  <pad>      o\n",
       "60415  <pad>      o\n",
       "\n",
       "[60416 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like pad tokens are included here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60078</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nodes preds\n",
       "0       <s>     o\n",
       "60078  </s>     o"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"nodes == '<s>' | nodes == '</s>'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df_based_on_start_and_end_token = df.query(\"nodes == '<s>' | nodes == '</s>'\")\n",
    "start_idx = filter_df_based_on_start_and_end_token.index.min() + 1\n",
    "end_idx = filter_df_based_on_start_and_end_token.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60078)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = df.iloc[start_idx:end_idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EX</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>s_ssn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60073</th>\n",
       "      <td>An</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60074</th>\n",
       "      <td>nex</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60075</th>\n",
       "      <td>I</td>\n",
       "      <td>b_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60076</th>\n",
       "      <td>-</td>\n",
       "      <td>e_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60077</th>\n",
       "      <td>4</td>\n",
       "      <td>e_n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60077 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nodes  preds\n",
       "1        EX      o\n",
       "2         -    s_n\n",
       "3         2  s_ssn\n",
       "4         .      o\n",
       "5         1      o\n",
       "...     ...    ...\n",
       "60073    An    s_n\n",
       "60074   nex    s_n\n",
       "60075     I    b_n\n",
       "60076     -    e_n\n",
       "60077     4    e_n\n",
       "\n",
       "[60077 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xpaths</th>\n",
       "      <th>text</th>\n",
       "      <th>xpaths_list</th>\n",
       "      <th>text_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60072</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60073</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>nex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60074</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60075</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60076</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60077 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  xpaths       text  \\\n",
       "0                               /html/body/document/type     EX-2.1   \n",
       "1                               /html/body/document/type     EX-2.1   \n",
       "2                               /html/body/document/type     EX-2.1   \n",
       "3                               /html/body/document/type     EX-2.1   \n",
       "4                               /html/body/document/type     EX-2.1   \n",
       "...                                                  ...        ...   \n",
       "60072  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60073  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60074  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60075  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60076  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "\n",
       "                                             xpaths_list text_list  \n",
       "0                               /html/body/document/type        EX  \n",
       "1                               /html/body/document/type         -  \n",
       "2                               /html/body/document/type         2  \n",
       "3                               /html/body/document/type         .  \n",
       "4                               /html/body/document/type         1  \n",
       "...                                                  ...       ...  \n",
       "60072  /html/body/document/type/sequence/filename/des...        An  \n",
       "60073  /html/body/document/type/sequence/filename/des...       nex  \n",
       "60074  /html/body/document/type/sequence/filename/des...         I  \n",
       "60075  /html/body/document/type/sequence/filename/des...         -  \n",
       "60076  /html/body/document/type/sequence/filename/des...         4  \n",
       "\n",
       "[60077 rows x 4 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploded_partial = exploded.iloc[:len(preds)].copy().reset_index(drop=True)\n",
    "#exploded_partial\n",
    "exploded_partial = exploded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xpaths</th>\n",
       "      <th>text</th>\n",
       "      <th>xpaths_list</th>\n",
       "      <th>text_list</th>\n",
       "      <th>nodes</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX</td>\n",
       "      <td>EX</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>s_ssn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>EX-2.1</td>\n",
       "      <td>/html/body/document/type</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60072</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>An</td>\n",
       "      <td>An</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60073</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>nex</td>\n",
       "      <td>nex</td>\n",
       "      <td>s_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60074</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>b_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60075</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>e_n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60076</th>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>Annex I-4</td>\n",
       "      <td>/html/body/document/type/sequence/filename/des...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e_n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60077 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  xpaths       text  \\\n",
       "0                               /html/body/document/type     EX-2.1   \n",
       "1                               /html/body/document/type     EX-2.1   \n",
       "2                               /html/body/document/type     EX-2.1   \n",
       "3                               /html/body/document/type     EX-2.1   \n",
       "4                               /html/body/document/type     EX-2.1   \n",
       "...                                                  ...        ...   \n",
       "60072  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60073  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60074  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60075  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "60076  /html/body/document/type/sequence/filename/des...  Annex I-4   \n",
       "\n",
       "                                             xpaths_list text_list nodes  \\\n",
       "0                               /html/body/document/type        EX    EX   \n",
       "1                               /html/body/document/type         -     -   \n",
       "2                               /html/body/document/type         2     2   \n",
       "3                               /html/body/document/type         .     .   \n",
       "4                               /html/body/document/type         1     1   \n",
       "...                                                  ...       ...   ...   \n",
       "60072  /html/body/document/type/sequence/filename/des...        An    An   \n",
       "60073  /html/body/document/type/sequence/filename/des...       nex   nex   \n",
       "60074  /html/body/document/type/sequence/filename/des...         I     I   \n",
       "60075  /html/body/document/type/sequence/filename/des...         -     -   \n",
       "60076  /html/body/document/type/sequence/filename/des...         4     4   \n",
       "\n",
       "       preds  \n",
       "0          o  \n",
       "1        s_n  \n",
       "2      s_ssn  \n",
       "3          o  \n",
       "4          o  \n",
       "...      ...  \n",
       "60072    s_n  \n",
       "60073    s_n  \n",
       "60074    b_n  \n",
       "60075    e_n  \n",
       "60076    e_n  \n",
       "\n",
       "[60077 rows x 6 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_partial['nodes'] = preds['nodes'].to_list()\n",
    "exploded_partial['preds'] = preds['preds'].to_list()\n",
    "exploded_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(exploded_partial.text_list == exploded_partial.nodes) == len(exploded_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(exploded_partial['xpaths'] == exploded_partial['xpaths_list']).sum() == len(exploded_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#exploded_partial[['xpaths','text','nodes','preds']].to_json('testing_contract_81.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_partial[['xpaths','text','nodes','preds']].to_csv('testing_contract_85.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:markupmna]",
   "language": "python",
   "name": "conda-env-markupmna-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09500e44e8424bda9c121da49ebeb1df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ee906c2608a4712bc63eda81b172809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19749a6a48cc4a1d91abff0fe43df1b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4fde29a12a8743d99293170cf49671ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eefd6b2dbaec4c978ce6d5b81bb91d0c",
      "max": 225,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_19749a6a48cc4a1d91abff0fe43df1b6",
      "value": 0
     }
    },
    "517ebaa35e2d42ba9837c35fcd6eee84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ee906c2608a4712bc63eda81b172809",
      "placeholder": "​",
      "style": "IPY_MODEL_09500e44e8424bda9c121da49ebeb1df",
      "value": "  0%"
     }
    },
    "56a6dc9eac5b41a494e636bb685be1ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "632672424cc04daf8a5ee6b5d7b23809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93c6897eb85e4240b05f446999e21353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_517ebaa35e2d42ba9837c35fcd6eee84",
       "IPY_MODEL_4fde29a12a8743d99293170cf49671ba",
       "IPY_MODEL_d15852a8683c4058b77c0c571a426f35"
      ],
      "layout": "IPY_MODEL_56a6dc9eac5b41a494e636bb685be1ae"
     }
    },
    "bfdd1b97a07c46919ff0fb6f1b580e47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d15852a8683c4058b77c0c571a426f35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_632672424cc04daf8a5ee6b5d7b23809",
      "placeholder": "​",
      "style": "IPY_MODEL_bfdd1b97a07c46919ff0fb6f1b580e47",
      "value": " 0/225 [00:00&lt;?, ?it/s]"
     }
    },
    "eefd6b2dbaec4c978ce6d5b81bb91d0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
